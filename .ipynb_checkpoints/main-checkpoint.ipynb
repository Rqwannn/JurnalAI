{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1265621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\users\\raqwan\\anaconda3\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\raqwan\\appdata\\roaming\\python\\python310\\site-packages (from pdf2image) (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574bd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb89b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\raqwan\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\raqwan\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af93673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Extract image\n",
    "import pytesseract\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, TfidfModel\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lda visual\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# PDF \n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Clean Data\n",
    "from bs4 import BeautifulSoup # 4.6.3\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8ba22",
   "metadata": {},
   "source": [
    "# Step 1, OCR\n",
    "## Extrak Text From PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5298d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path ke file PDF\n",
    "pdf_file = \"assets/jurnal.pdf\"\n",
    "result = []\n",
    "\n",
    "# Fungsi untuk mengekstrak teks dari file PDF dan menampilkan contour\n",
    "def extract_text_and_show_contours(pdf_path):\n",
    "    \n",
    "    # Ubah file PDF menjadi gambar (gambar per halaman)\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    try:\n",
    "        images = convert_pdf_to_images(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Loop melalui setiap gambar (halaman) dan ekstrak teks serta tampilkan contour\n",
    "    for i, image in enumerate(images):\n",
    "        text = show_text_contours(image, i)\n",
    "        print(f\"Hasil ekstraksi teks dari halaman {i + 1}\")\n",
    "        \n",
    "#         show_text_contours(image)\n",
    "\n",
    "# Fungsi untuk mengubah file PDF menjadi daftar gambar\n",
    "def convert_pdf_to_images(pdf_path):\n",
    "    images = []\n",
    "\n",
    "    # Fungsi untuk mengonversi PDF menjadi gambar\n",
    "    pdf_images = convert_from_path(pdf_path)\n",
    "\n",
    "    # Simpan setiap gambar sebagai file gambar terpisah\n",
    "    for i, image in enumerate(pdf_images):\n",
    "        images.append(f\"convert_to_img/page_{i + 1}.png\")\n",
    "        image.save(f'convert_to_img/page_{i + 1}.png', 'PNG')\n",
    "                \n",
    "    return images\n",
    "\n",
    "# Fungsi untuk mengekstrak teks dari gambar menggunakan pytesseract\n",
    "def extract_text_from_image(image):\n",
    "\n",
    "    ocr_result = pytesseract.image_to_string(image)\n",
    "    ocr_result = ocr_result.split(\"\\n\")\n",
    "\n",
    "    for line in ocr_result:\n",
    "\n",
    "        result.append(line)\n",
    "                \n",
    "    return ocr_result\n",
    "\n",
    "# Fungsi untuk menampilkan contour dari gambar\n",
    "def show_text_contours(image, index_img):\n",
    "    image = cv2.imread(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (7,7), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    kernal = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 50))\n",
    "    dilate = cv2.dilate(thresh, kernal, iterations=1)\n",
    "    \n",
    "    cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    cnts = sorted(cnts, key=lambda x: cv2.boundingRect(x)[1])\n",
    "    \n",
    "    if_agains = 0\n",
    "        \n",
    "    for c in cnts:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "\n",
    "        # if statement berikut untuk mengambil bagian isi dari image, jika tidak maka semua kata akan di contours\n",
    "\n",
    "        if h > 200 and w > 250:\n",
    "            roi = image[y:y+h, x:x+w]\n",
    "            \n",
    "            if if_agains > 0:\n",
    "                cv2.imwrite(f\"temp/jurnal_countor_{index_img}_{if_agains}.png\", roi)\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (36, 255, 12), 2)\n",
    "            else:\n",
    "                cv2.imwrite(f\"temp/jurnal_countor_{index_img}.png\", roi)\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (36, 255, 12), 2)\n",
    "            \n",
    "            extract_text_from_image(roi)\n",
    "            if_agains += 1\n",
    "                \n",
    "    cv2.imwrite(f\"boxes/sample_boxes_{index_img}.png\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd9a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_and_show_contours(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5769d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ceddfd",
   "metadata": {},
   "source": [
    "# Step 2, Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    data = BeautifulSoup(data, \"lxml\").get_text()\n",
    "    \n",
    "    # Delete the @\n",
    "    data = re.sub(r\"@[A-Za-z0-9]+\", ' ', data)\n",
    "    \n",
    "    # Delete URL links\n",
    "    data = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', data)\n",
    "    \n",
    "    # Just keep letters and important punctuation\n",
    "    data = re.sub(r\"[^a-zA-Z.!?']\", ' ', data)\n",
    "    \n",
    "    # Just keep letters and important punctuation\n",
    "    # data = re.sub('(?i)[^0-9a-z.!?]', '', data)\n",
    "    \n",
    "#     data = re.sub(r'[.,;]', '', data)\n",
    "    \n",
    "    # Remove additional spaces\n",
    "    data = re.sub(r\" +\", ' ', data)\n",
    "    \n",
    "    # Remove spaces at the beginning and end of text\n",
    "    data = data.strip()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = []\n",
    "\n",
    "\n",
    "for item in result:\n",
    "    item = item.strip()\n",
    "    data = clean_data(item)\n",
    "        \n",
    "    if len(data) > 4:\n",
    "        data_clean.append(data)\n",
    "\n",
    "print(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e0033",
   "metadata": {},
   "source": [
    "# Step 3, Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339729b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\", \"PROPN\", \"WORK_OF_ART\", \"ORG\", \"GPE\"]\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\", \"PROPN\"]):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    \n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "                \n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "        \n",
    "    return (texts_out)\n",
    "\n",
    "\n",
    "lemmatized_texts = lemmatization(data_clean)\n",
    "lemmatized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_words(texts):\n",
    "    final = []\n",
    "    \n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "        \n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95294b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIGRAMS AND TRIGRAMS\n",
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words], threshold=100)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return([bigram[doc] for doc in texts])\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "data_bigrams_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45af242",
   "metadata": {},
   "source": [
    "# Step 3, Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF REMOVAL\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# print (corpus[0][0:20])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words  = []\n",
    "words_missing_in_tfidf = []\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus[:-1],\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = corpus[-1]\n",
    "\n",
    "vector = lda_model[test_doc]\n",
    "\n",
    "def Sort(sub_li):\n",
    "    sub_li.sort(key = lambda x: x[1])\n",
    "    sub_li.reverse()\n",
    "    return (sub_li)\n",
    "\n",
    "new_vector = Sort(vector)\n",
    "\n",
    "print (new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076db784",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(\"models/test_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b368e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = gensim.models.ldamodel.LdaModel.load(\"models/test_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = corpus[-1]\n",
    "\n",
    "vector = new_model[test_doc]\n",
    "\n",
    "def Sort(sub_li):\n",
    "    sub_li.sort(key = lambda x: x[1])\n",
    "    sub_li.reverse()\n",
    "    return (sub_li)\n",
    "\n",
    "new_vector = Sort(vector)\n",
    "print (new_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d5384",
   "metadata": {},
   "source": [
    "# Step 4, Visual the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a337029",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan koordinat titik visualisasi\n",
    "\n",
    "data_to_save = {\n",
    "    \"topic_coordinates\": vis.topic_coordinates.to_dict(orient=\"split\"),\n",
    "    \"topic_info\": vis.topic_info.to_dict(orient=\"split\"),\n",
    "    \"token_table\": vis.token_table.to_dict(orient=\"split\"),\n",
    "    \"topic_order\": vis.topic_order\n",
    "}\n",
    "\n",
    "# Simpan data ke dalam file JSON\n",
    "\n",
    "with open(\"outputs/visualisasi_lda.json\", \"w\") as json_file:\n",
    "    json.dump(data_to_save, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565db63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff2857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96248e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
